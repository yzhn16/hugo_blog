<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
    <title>李宏毅ML课程笔记——Self-attention - yzhn&#39;s Notes</title><meta name="author" content="">
<meta name="author-link" content="">
<meta name="description" content="李宏毅2021/2022春机器学习课程——自注意力机制（Self-attention）" />
<meta name="keywords" content="" /><meta itemprop="name" content="李宏毅ML课程笔记——Self-attention">
<meta itemprop="description" content="李宏毅2021/2022春机器学习课程——自注意力机制（Self-attention）"><meta itemprop="datePublished" content="2022-04-23T17:45:00+00:00" />
<meta itemprop="dateModified" content="2022-04-23T17:45:00+00:00" />
<meta itemprop="wordCount" content="2411"><meta itemprop="image" content="/logo.png"/>
<meta itemprop="keywords" content="" /><meta property="og:title" content="李宏毅ML课程笔记——Self-attention" />
<meta property="og:description" content="李宏毅2021/2022春机器学习课程——自注意力机制（Self-attention）" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-attention/" /><meta property="og:image" content="/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-23T17:45:00+00:00" />
<meta property="article:modified_time" content="2022-04-23T17:45:00+00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="/logo.png"/>

<meta name="twitter:title" content="李宏毅ML课程笔记——Self-attention"/>
<meta name="twitter:description" content="李宏毅2021/2022春机器学习课程——自注意力机制（Self-attention）"/>
<meta name="application-name" content="FixIt">
<meta name="apple-mobile-web-app-title" content="FixIt"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#252627"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-attention/" /><link rel="prev" href="/hexo%E4%B8%ADnext%E4%B8%BB%E9%A2%98%E4%B8%8B%E5%9F%BA%E4%BA%8Efancybox%E7%9A%84%E7%9B%B8%E5%86%8C%E9%A1%B5%E9%9D%A2%E5%AE%9E%E7%8E%B0/" /><link rel="next" href="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0transformer/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "李宏毅ML课程笔记——Self-attention",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "\/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-attention\/"
    },"genre": "posts","wordcount":  2411 ,
    "url": "\/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-attention\/","datePublished": "2022-04-23T17:45:00+00:00","dateModified": "2022-04-23T17:45:00+00:00","publisher": {
      "@type": "Organization",
      "name": ""},"author": {
        "@type": "Person",
        "name": "作者"
      },"description": ""
  }
  </script></head>
  <body header-desktop="sticky" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script><div class="wrapper"><header class="desktop" id="header-desktop">
  <div class="header-wrapper" github-corner="right">
    <div class="header-title">
      <a href="/" title="yzhn&#39;s Notes"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/images/fixit.svg"
    data-srcset="/images/fixit.svg, /images/fixit.svg 1.5x, /images/fixit.svg 2x"
    data-sizes="auto"
    alt="yzhn&#39;s Notes"
    title="yzhn&#39;s Notes" /><span class="header-title-text">yzhn&#39;s Notes</span></a><span class="header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li
              class="menu-item"
              
            >
              <a
                class="menu-link"
                href="/posts/"
                
                
              >文章</a></li><li
              class="menu-item"
              
            >
              <a
                class="menu-link"
                href="/categories/"
                
                
              >分类</a></li><li
              class="menu-item"
              
            >
              <a
                class="menu-link"
                href="/tags/"
                
                
              >标签</a></li><li
              class="menu-item"
              
            >
              <a
                class="menu-link"
                href="/about"
                
                
              >关于</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="yzhn&#39;s Notes"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/images/fixit.svg"
    data-srcset="/images/fixit.svg, /images/fixit.svg 1.5x, /images/fixit.svg 2x"
    data-sizes="auto"
    alt="/images/fixit.svg"
    title="/images/fixit.svg" /><span class="header-title-text">yzhn&#39;s Notes</span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                class="menu-link"
                href="/posts/"
                
                
              >文章</a></li><li
              class="menu-item"
            ><a
                class="menu-link"
                href="/categories/"
                
                
              >分类</a></li><li
              class="menu-item"
            ><a
                class="menu-link"
                href="/tags/"
                
                
              >标签</a></li><li
              class="menu-item"
            ><a
                class="menu-link"
                href="/about"
                
                
              >关于</a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw"></i>
        </li></ul>
    </nav>
  </div>
</header>
<div class="search-dropdown desktop">
  <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
  <div id="search-dropdown-mobile"></div>
</div>
<main class="container-reverse" page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录</h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom">
    
  </aside>

  <article class="page single"><h1 class="single-title animate__animated animate__flipInX">李宏毅ML课程笔记——Self-attention</h1><div class="post-meta">
      
      <div class="post-meta-line"><span title=2022-04-23&#32;17:45:00>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="April 23, 2022" >April 23, 2022</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 2411 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 5 分钟&nbsp;</div>
    </div><div class="details toc" id="toc-static" kept="false">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#将一排向量作为输入">将一排向量作为输入</a>
      <ul>
        <li><a href="#方法">方法</a></li>
        <li><a href="#输入">输入</a></li>
        <li><a href="#输出">输出</a>
          <ul>
            <li><a href="#n个向量对应n个标签">N个向量对应N个标签</a></li>
            <li><a href="#n个向量对应一个标签">N个向量对应一个标签</a></li>
            <li><a href="#n个向量对应多个标签seq2seq">N个向量对应多个标签（seq2seq）</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#fully-connected-network">Fully-connected Network</a></li>
    <li><a href="#self-attention">Self-attention</a>
      <ul>
        <li><a href="#如何生成b1">如何生成$b^1$？</a>
          <ul>
            <li><a href="#a1与其他向量的相关性alpha的计算方法">$a^1$与其他向量的相关性$\alpha$的计算方法</a>
              <ul>
                <li><a href="#dot-product"><strong>Dot-product</strong></a></li>
                <li><a href="#additive">Additive</a></li>
              </ul>
            </li>
            <li><a href="#b1的计算">$b^1$的计算</a></li>
          </ul>
        </li>
        <li><a href="#输出向量组b1b2b3b4的完整计算过程">输出向量组${b^1,b^2,b^3,b^4}$的完整计算过程</a>
          <ul>
            <li><a href="#qkv的计算">$Q$、$K$、$V$的计算</a></li>
            <li><a href="#attention分数的计算">Attention分数的计算</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li><a href="#总结">总结</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#multi-head-self-attention">Multi-head Self-attention</a></li>
    <li><a href="#位置编码">位置编码</a></li>
    <li><a href="#self-attention用于语音">Self-attention用于语音</a></li>
    <li><a href="#self-attention用于图像">Self-attention用于图像</a></li>
    <li><a href="#self-attention-vs-cnn">Self-attention v.s. CNN</a></li>
    <li><a href="#self-attention-vs-rnn">Self-attention v.s. RNN</a></li>
    <li><a href="#self-attention用于图">Self-attention用于图</a></li>
    <li><a href="#more">More</a></li>
  </ul>
</nav></div>
      </div><div class="content" id="content"><p><a
  href="https://www.bilibili.com/video/BV1Wv411h7kN?p=38"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>李宏毅2021/2022春机器学习课程——自注意力机制（Self-attention）</a></p>
<h1 id="自注意力机制self-attention">自注意力机制（Self-Attention）</h1>
<p>考虑两种不同的输入：</p>
<ul>
<li>输入是一个向量</li>
<li><strong>输入是一排向量</strong>（输入的向量个数可能会改变）</li>
</ul>
<h2 id="将一排向量作为输入">将一排向量作为输入</h2>
<h3 id="方法">方法</h3>
<ul>
<li>One-hot Encoding</li>
<li>Word Embedding</li>
</ul>
<h3 id="输入">输入</h3>
<ul>
<li>一段语音窗口</li>
<li>一张图</li>
<li>分子结构</li>
<li>&hellip;</li>
</ul>
<h3 id="输出">输出</h3>
<h4 id="n个向量对应n个标签">N个向量对应N个标签</h4>
<ul>
<li>句子中每个词的词性：I saw a saw -&gt; N V DET N</li>
<li>社交网络中每个人的购买意向：甲 -&gt; buy;乙 -&gt; not</li>
</ul>
<h4 id="n个向量对应一个标签">N个向量对应一个标签</h4>
<ul>
<li>情感分析：this is good -&gt; positive</li>
<li>分子属性分析：一个分子图 -&gt; 亲水性</li>
</ul>
<h4 id="n个向量对应多个标签seq2seq">N个向量对应多个标签（seq2seq）</h4>
<ul>
<li>机器翻译</li>
</ul>
<h2 id="fully-connected-network">Fully-connected Network</h2>
<p>使用全连接网络，设置窗口大小，每次输入邻近的多个词。但限制于窗口大小，无法考虑整个句子的影响，且窗口覆盖整个句子比较困难。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/fc.png" style="zoom:50%;" />
<h2 id="self-attention">Self-attention</h2>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention.png" style="zoom:50%;" />
<p>例：</p>
<ul>
<li>输入：一排向量${a^1,a^2,a^3,a^4}$</li>
<li>输出：一排向量${b^1,b^2,b^3,b^4}$</li>
</ul>
<p><em>$b^1$、$b^2$、$b^3$、$b^4$分别都是考虑了$a^1$、$a^2$、$a^3$、$a^4$而产生的。</em></p>
<h3 id="如何生成b1">如何生成$b^1$？</h3>
<h4 id="a1与其他向量的相关性alpha的计算方法">$a^1$与其他向量的相关性$\alpha$的计算方法</h4>
<h5 id="dot-product"><strong>Dot-product</strong></h5>
<p>输入两个向量，分别与矩阵$W^q$和$W^k$相乘，再将得到的向量$q$和$k$做点乘。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/dot-product.png" style="zoom:50%;" />
$$
\begin{aligned}
q&=a^1\times W^q \\
k&=a^2\times W^k \\
\alpha &= q\cdot k
\end{aligned}
$$
<h5 id="additive">Additive</h5>
<p>输入两个向量，分别与矩阵$W^q$和$W^k$相乘，将得到的向量$q$和$k$串起来并通过激活函数，最后通过一个变换得到$alpha$。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/additive.png" style="zoom:50%;" />
<h4 id="b1的计算">$b^1$的计算</h4>
<p>本节中的例子中，对于$b^1$，计算步骤如下：</p>
<ul>
<li>step1.计算$q^1$</li>
</ul>
<p>$$
q^1=W^qa^1
$$</p>
<ul>
<li>step2.计算$k^1$、$k^2$、$k^3$、$k^4$</li>
</ul>
<p>$$
k^i=W^ka^i
$$</p>
<ul>
<li>step3.计算$\alpha_{1,1}$、$\alpha_{1,2}$、$\alpha_{1,3}$、$\alpha_{1,4}$</li>
</ul>
<p>$$
\alpha_{1,i}=q^1\cdot k^i \
$$</p>
<ul>
<li>step4.通过Soft-max（也可使用ReLU等）计算$\alpha_{1,1}&rsquo;$、$\alpha_{1,2}&rsquo;$、$\alpha_{1,3}&rsquo;$、$\alpha_{1,4}'$</li>
</ul>
<p>$$
\alpha_{1,i}&rsquo;=\frac{\exp(\alpha_{1,i})}{\sum_j\exp(\alpha_{1,j})}
$$</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention-1.png" style="zoom:50%;" />
<ul>
<li>
<p>step5.计算$v^1$、$v^2$、$v^3$、$v^4$
$$
v^i=W^va^i
$$</p>
</li>
<li>
<p>step6.计算$b^1$（根据Attenion分数抽取重要信息）
$$
b^1=\sum_j\alpha_{1,j}&lsquo;v^j
$$</p>
</li>
</ul>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention-2.png" style="zoom:50%;" />
<h3 id="输出向量组b1b2b3b4的完整计算过程">输出向量组${b^1,b^2,b^3,b^4}$的完整计算过程</h3>
<p>整理以上过程，$Q$、$K$、$V$和Attention分数的计算过程如下：</p>
<h4 id="qkv的计算">$Q$、$K$、$V$的计算</h4>
<p>由：
$$
\begin{aligned}
q^i &amp;=W^qa^i \
k^i &amp;=W^ka^i \
v^i &amp;=W^va^i
\end{aligned}
$$
有：
$$
\begin{aligned}
\begin{bmatrix}
q^1 &amp; q^2 &amp; q^3 &amp; q^4
\end{bmatrix}
&amp;=W^q
\begin{bmatrix}
a^1 &amp; a^2 &amp; a^3 &amp; a^4
\end{bmatrix}
\
\begin{bmatrix}
k^1 &amp; k^2 &amp; k^3 &amp; k^4
\end{bmatrix}
&amp;=W^k
\begin{bmatrix}
a^1 &amp; a^2 &amp; a^3 &amp; a^4
\end{bmatrix}
\
\begin{bmatrix}
v^1 &amp; v^2 &amp; v^3 &amp; v^4
\end{bmatrix}
&amp;=W^v
\begin{bmatrix}
a^1 &amp; a^2 &amp; a^3 &amp; a^4
\end{bmatrix}
\end{aligned}
$$
即：
$$
\begin{aligned}
Q&amp;=W^qI \
K&amp;=W^kI \
V&amp;=W^vI
\end{aligned}
$$
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention-3.png" style="zoom:50%;" /></p>
<h4 id="attention分数的计算">Attention分数的计算</h4>
<h1 id="endbmatrix">由：
$$
\alpha_{1,i}=k^i\cdot q^1 \
$$
有：
$$
\begin{bmatrix}
\alpha_{1,1} \
\alpha_{1,2} \
\alpha_{1,3} \
\alpha_{1,4}
\end{bmatrix}</h1>
<h1 id="endbmatrix-1">\begin{bmatrix}
k^1 \
k^2 \
k^3 \
k^4
\end{bmatrix}
\cdot
q^1
$$
进一步有：
$$
\begin{bmatrix}
\alpha_{1,1} &amp; \alpha_{2,1} &amp; \alpha_{3,1} &amp;\alpha_{4,1} \
\alpha_{1,2} &amp; \alpha_{2,2} &amp; \alpha_{3,2} &amp;\alpha_{4,2} \
\alpha_{1,3} &amp; \alpha_{2,3} &amp; \alpha_{3,3} &amp;\alpha_{4,3} \
\alpha_{1,4} &amp; \alpha_{2,4} &amp; \alpha_{3,4} &amp;\alpha_{4,4} \
\end{bmatrix}</h1>
<p>\begin{bmatrix}
k^1 \
k^2 \
k^3 \
k^4
\end{bmatrix}
\cdot
\begin{bmatrix}
q^1 &amp; q^2 &amp; q^3 &amp;q^4
\end{bmatrix}
$$
即：
$$
A=K^TQ
$$
对$A$进行Soft-max得到$A&rsquo;$：
$$
A&rsquo;=
\begin{bmatrix}
\alpha_{1,1}&rsquo; &amp; \alpha_{2,1}&rsquo; &amp; \alpha_{3,1}&rsquo; &amp;\alpha_{4,1}&rsquo; \
\alpha_{1,2}&rsquo; &amp; \alpha_{2,2}&rsquo; &amp; \alpha_{3,2}&rsquo; &amp;\alpha_{4,2}&rsquo; \
\alpha_{1,3}&rsquo; &amp; \alpha_{2,3}&rsquo; &amp; \alpha_{3,3}&rsquo; &amp;\alpha_{4,3}&rsquo; \
\alpha_{1,4}&rsquo; &amp; \alpha_{2,4}&rsquo; &amp; \alpha_{3,4}&rsquo; &amp;\alpha_{4,4}&rsquo; \
\end{bmatrix}
$$
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention-4.png" style="zoom:50%;" /></p>
<h1 id="endbmatrix-2">最后将$V$与$A&rsquo;$相乘，得到$b^1$、$b^2$、$b^3$、$b^4$：
$$
\begin{bmatrix}
b^1 &amp; b^2 &amp; b^3 &amp; b^4
\end{bmatrix}</h1>
<p>\begin{bmatrix}
v^1 &amp; v^2 &amp; v^3 &amp; v^4
\end{bmatrix}
\cdot
\begin{bmatrix}
\alpha_{1,1}&rsquo; &amp; \alpha_{2,1}&rsquo; &amp; \alpha_{3,1}&rsquo; &amp;\alpha_{4,1}&rsquo; \
\alpha_{1,2}&rsquo; &amp; \alpha_{2,2}&rsquo; &amp; \alpha_{3,2}&rsquo; &amp;\alpha_{4,2}&rsquo; \
\alpha_{1,3}&rsquo; &amp; \alpha_{2,3}&rsquo; &amp; \alpha_{3,3}&rsquo; &amp;\alpha_{4,3}&rsquo; \
\alpha_{1,4}&rsquo; &amp; \alpha_{2,4}&rsquo; &amp; \alpha_{3,4}&rsquo; &amp;\alpha_{4,4}&rsquo; \
\end{bmatrix}
$$
即得到Self-attention的输出：
$$
O=VA&rsquo;
$$</p>
<h4 id="总结">总结</h4>
<p>$$
\begin{aligned}
&amp;Q=W^qI \
&amp;K=W^kI \
&amp;V=W^vI \
&amp;A=K^TQ \
&amp;A \rightarrow A&rsquo; \
&amp;O=VA'
\end{aligned}
$$</p>
<p>$W^q$、$W^k$、$W^v$是需要学习的参数。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention-5.png" style="zoom:50%;" />
<h2 id="multi-head-self-attention">Multi-head Self-attention</h2>
<p>多个$q$，对应不同种类的相关性。</p>
<p>例如对于2 head的情况，$a^i$对应的$q^i$、$k^i$、$v^i$具体有$q^{i,1}$、$k^{i,1}$、$v^{i,1}$：
$$
\begin{aligned}
&amp;q^{i,1}=W^{q,1}q^i \
&amp;q^{i,2}=W^{q,2}q^i
\end{aligned}
$$
类似的，$a^j$对应的$q^j$、$k^j$、$v^j$具体有$q^{j,1}$、$k^{j,1}$、$v^{j,1}$：</p>
<p>在使用Dot-product计算Attention分数时，使用对应的$q$和$k$进行计算，$q^{i,1}$分别和$k^{i,1}$、$k^{j,1}$进行计算，在将Attention分数分别和$v^{i,1}$、$v^{j,1}$计算，得到$b^{i,1}$，类似可得到$b^{i,2}$。
$$
b^i=W^O
\begin{bmatrix}
b^{i,1} \
b^{i,2}
\end{bmatrix}
$$
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/multi-head%20self-attention.png" style="zoom:50%;" /></p>
<h2 id="位置编码">位置编码</h2>
<p>Self-attention中，对输入的几个向量所进行的操作是相同的，与位置无关，可能丢失了位置信息。</p>
<p>为每一个位置都设定一个向量$e^i$，将该向量加到$a^i$上。
$$
e^i+a^i\rightarrow q^i,k^i,v^i
$$
向量$e^i$可以通过一个规则设定（人工）或从训练数据中学习出来。</p>
<p>有各种不同的方法产生位置编码：</p>
<ul>
<li>Sinusoidal</li>
<li>Position embedding</li>
<li>FLOATER</li>
<li>RNN</li>
<li>&hellip;</li>
</ul>
<h2 id="self-attention用于语音">Self-attention用于语音</h2>
<p>把声音讯号表示为一排向量，一般一个向量只有10ms的长度，会导致向量个数过多，$A&rsquo;$计算的复杂度是$O(L^2)$，一般使用<strong>Truncated Self-attention</strong>，不看一整句话，只看一小部分。</p>
<h2 id="self-attention用于图像">Self-attention用于图像</h2>
<p>一张图片可以看成是一排向量。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention%20for%20img.png" style="zoom:50%;" />
<ul>
<li>Self-Attention GAN</li>
<li>Detection Transformer(DETR)</li>
</ul>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention%20for%20img2.png" style="zoom:50%;" />
<h2 id="self-attention-vs-cnn">Self-attention v.s. CNN</h2>
<p>Self-attention可以看作复杂化的CNN，Self-attention考虑全局。CNN是Self-attention的特例，Self-attention可以通过设定合适的参数，达到和CNN同样的效果。</p>
<ul>
<li>[<a
  href="https://arxiv.org/abs/1911.03584"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>1911.03584] On the Relationship between Self-Attention and Convolutional Layers (arxiv.org)</a></li>
</ul>
<h2 id="self-attention-vs-rnn">Self-attention v.s. RNN</h2>
<p>RNN（Recurrent Neural Network）：</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/4/self-attention%20vs%20RNN.png" style="zoom:50%;" />
<p>RNN的缺点很明显，很难去考虑到输入位置较远的向量，并且无法并行计算。</p>
<ul>
<li>[<a
  href="https://arxiv.org/abs/2006.16236"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>2006.16236] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (arxiv.org)</a></li>
</ul>
<h2 id="self-attention用于图">Self-attention用于图</h2>
<p>图中每个结点可以表示为一个向量，边可以用来考虑结点之间的关联性，计算Attention分数时，只需计算相连的结点。</p>
<p>Self-attention用在图上面，是某一种类型的GNN（Graph Neural Network）。</p>
<h2 id="more">More</h2>
<p>Self-Attention的计算量较大，优化效率是一个研究方向。</p>
<ul>
<li>[<a
  href="https://arxiv.org/abs/2011.04006"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>2011.04006] Long Range Arena: A Benchmark for Efficient Transformers (arxiv.org)</a></li>
<li>[<a
  href="https://arxiv.org/abs/2009.06732"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>2009.06732] Efficient Transformers: A Survey (arxiv.org)</a></li>
</ul></div><div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2022-04-23&#32;17:45:00>更新于 April 23, 2022</span>
      </div>
      <div class="post-info-license"><span><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
    </div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a
  href="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-attention/index.md"
  
    title="阅读原始文档"
  
  
  
  
  
    class="link-to-markdown"
  
  
>阅读原始文档</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-attention/" data-title="李宏毅ML课程笔记——Self-attention"><i class="fa-brands fa-twitter fa-fw"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-attention/"><i class="fa-brands fa-facebook-square fa-fw"></i></a>
  <a href="javascript:void(0);" title="分享到 WhatsApp" data-sharer="whatsapp" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-attention/" data-title="李宏毅ML课程笔记——Self-attention" data-web><i class="fa-brands fa-whatsapp fa-fw"></i></a>
  <a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-attention/" data-title="李宏毅ML课程笔记——Self-attention"><i data-svg-src="/lib/simple-icons/icons/line.min.svg"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-attention/" data-title="李宏毅ML课程笔记——Self-attention"><i class="fa-brands fa-weibo fa-fw"></i></a>
  <a href="javascript:void(0);" title="分享到 Myspace" data-sharer="myspace" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-attention/" data-title="李宏毅ML课程笔记——Self-attention" data-description=""><i data-svg-src="/lib/simple-icons/icons/myspace.min.svg"></i></a>
  <a href="javascript:void(0);" title="分享到 Blogger" data-sharer="blogger" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-attention/" data-title="李宏毅ML课程笔记——Self-attention" data-description=""><i class="fa-brands fa-blogger fa-fw"></i></a>
  <a href="javascript:void(0);" title="分享到 Evernote" data-sharer="evernote" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-attention/" data-title="李宏毅ML课程笔记——Self-attention"><i class="fa-brands fa-evernote fa-fw"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/hexo%E4%B8%ADnext%E4%B8%BB%E9%A2%98%E4%B8%8B%E5%9F%BA%E4%BA%8Efancybox%E7%9A%84%E7%9B%B8%E5%86%8C%E9%A1%B5%E9%9D%A2%E5%AE%9E%E7%8E%B0/" class="prev" rel="prev" title="Hexo中NexT主题下基于Fancybox的相册页面实现"><i class="fa-solid fa-angle-left fa-fw"></i>Hexo中NexT主题下基于Fancybox的相册页面实现</a>
      <a href="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0transformer/" class="next" rel="next" title="李宏毅ML课程笔记——Transformer">李宏毅ML课程笔记——Transformer<i class="fa-solid fa-angle-right fa-fw"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">
          由<a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreffer" title="Hugo %v">Hugo</a>
          ×
          <a href="https://github.com/Lruihao/FixIt" target="_blank" rel="external nofollow noopener noreffer" title="FixIt %v"><img class="fixit-icon" src="/images/fixit.svg" alt="FixIt logo" />&nbsp;FixIt</a>
          强力驱动
          
        </div><div class="footer-line copyright"><i class="fa-regular fa-copyright fa-fw"></i>
            <span itemprop="copyrightYear">2020 - 2022</span><span class="license footer-divider"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line beian"><span class="icp footer-divider">鄂ICP备20012765号</span></div></div>
  </footer></div><div class="widgets">
  <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
      <i class="fa-solid fa-arrow-up fa-fw"></i>
    </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
      <i class="fa-solid fa-comment fa-fw"></i>
    </a>
  </div><a
  href="https://github.com/yzhn16"
  
    title="在 GitHub 上查看源代码"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
    class="github-corner right d-none-mobile"
  
  
><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div>
</div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js" defer></script><script type="text/javascript" src="/lib/lunr/lunr.min.js" defer></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js" defer></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js" defer></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js" async defer></script><script type="text/javascript" src="/lib/sharer/sharer.min.js" async defer></script><script type="text/javascript" src="/lib/katex/katex.min.js" defer></script><script type="text/javascript" src="/lib/katex/auto-render.min.js" defer></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js" defer></script><script type="text/javascript" src="/lib/katex/mhchem.min.js" defer></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"enablePWA":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js" defer></script><script type="text/javascript" src="/js/_custom.min.js" defer></script></body>
</html>
