<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
    <title>李宏毅ML课程笔记——Self-Supervised Learning - yzhn&#39;s Notes</title><meta name="author" content="">
<meta name="author-link" content="">
<meta name="description" content="
李宏毅2021/2022春机器学习课程——Self-Supervised Learning" />
<meta name="keywords" content="" /><meta itemprop="name" content="李宏毅ML课程笔记——Self-Supervised Learning">
<meta itemprop="description" content="
李宏毅2021/2022春机器学习课程——Self-Supervised Learning"><meta itemprop="datePublished" content="2022-05-16T18:08:00+00:00" />
<meta itemprop="dateModified" content="2022-05-16T18:08:00+00:00" />
<meta itemprop="wordCount" content="4552"><meta itemprop="image" content="/logo.png"/>
<meta itemprop="keywords" content="" /><meta property="og:title" content="李宏毅ML课程笔记——Self-Supervised Learning" />
<meta property="og:description" content="
李宏毅2021/2022春机器学习课程——Self-Supervised Learning" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-supervised-learning/" /><meta property="og:image" content="/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-16T18:08:00+00:00" />
<meta property="article:modified_time" content="2022-05-16T18:08:00+00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="/logo.png"/>

<meta name="twitter:title" content="李宏毅ML课程笔记——Self-Supervised Learning"/>
<meta name="twitter:description" content="
李宏毅2021/2022春机器学习课程——Self-Supervised Learning"/>
<meta name="application-name" content="FixIt">
<meta name="apple-mobile-web-app-title" content="FixIt"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#252627"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-supervised-learning/" /><link rel="prev" href="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "李宏毅ML课程笔记——Self-Supervised Learning",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "\/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-supervised-learning\/"
    },"genre": "posts","wordcount":  4552 ,
    "url": "\/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-supervised-learning\/","datePublished": "2022-05-16T18:08:00+00:00","dateModified": "2022-05-16T18:08:00+00:00","publisher": {
      "@type": "Organization",
      "name": ""},"author": {
        "@type": "Person",
        "name": "作者"
      },"description": ""
  }
  </script></head>
  <body header-desktop="sticky" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script><div class="wrapper"><header class="desktop" id="header-desktop">
  <div class="header-wrapper" github-corner="right">
    <div class="header-title">
      <a href="/" title="yzhn&#39;s Notes"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/images/fixit.svg"
    data-srcset="/images/fixit.svg, /images/fixit.svg 1.5x, /images/fixit.svg 2x"
    data-sizes="auto"
    alt="yzhn&#39;s Notes"
    title="yzhn&#39;s Notes" /><span class="header-title-text">yzhn&#39;s Notes</span></a><span class="header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li
              class="menu-item"
              
            >
              <a
                class="menu-link"
                href="/posts/"
                
                
              >文章</a></li><li
              class="menu-item"
              
            >
              <a
                class="menu-link"
                href="/categories/"
                
                
              >分类</a></li><li
              class="menu-item"
              
            >
              <a
                class="menu-link"
                href="/tags/"
                
                
              >标签</a></li><li
              class="menu-item"
              
            >
              <a
                class="menu-link"
                href="/about"
                
                
              >关于</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="yzhn&#39;s Notes"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/images/fixit.svg"
    data-srcset="/images/fixit.svg, /images/fixit.svg 1.5x, /images/fixit.svg 2x"
    data-sizes="auto"
    alt="/images/fixit.svg"
    title="/images/fixit.svg" /><span class="header-title-text">yzhn&#39;s Notes</span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                class="menu-link"
                href="/posts/"
                
                
              >文章</a></li><li
              class="menu-item"
            ><a
                class="menu-link"
                href="/categories/"
                
                
              >分类</a></li><li
              class="menu-item"
            ><a
                class="menu-link"
                href="/tags/"
                
                
              >标签</a></li><li
              class="menu-item"
            ><a
                class="menu-link"
                href="/about"
                
                
              >关于</a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw"></i>
        </li></ul>
    </nav>
  </div>
</header>
<div class="search-dropdown desktop">
  <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
  <div id="search-dropdown-mobile"></div>
</div>
<main class="container-reverse" page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录</h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom">
    
  </aside>

  <article class="page single"><h1 class="single-title animate__animated animate__flipInX">李宏毅ML课程笔记——Self-Supervised Learning</h1><div class="post-meta">
      
      <div class="post-meta-line"><span title=2022-05-16&#32;18:08:00>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="May 16, 2022" >May 16, 2022</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 4552 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 10 分钟&nbsp;</div>
    </div><div class="details toc" id="toc-static" kept="false">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#芝麻街与进击的巨人">芝麻街与进击的巨人</a>
      <ul>
        <li><a href="#芝麻街">芝麻街</a></li>
        <li><a href="#进击的巨人bertolt-hoover">进击的巨人：Bertolt Hoover</a></li>
        <li><a href="#主流模型参数量">主流模型参数量</a></li>
      </ul>
    </li>
    <li><a href="#bert">BERT</a>
      <ul>
        <li><a href="#self-supervised-learning">Self-supervised Learning</a></li>
        <li><a href="#masked-token-prediction">Masked token prediction</a></li>
        <li><a href="#next-sentence-prediction">Next Sentence Prediction</a></li>
        <li><a href="#downstream-tasks">Downstream Tasks</a></li>
        <li><a href="#glue">GLUE</a></li>
        <li><a href="#how-to-use-bert">How to use BERT</a>
          <ul>
            <li><a href="#case-1">Case 1</a></li>
            <li><a href="#case-2">Case 2</a></li>
            <li><a href="#case-3">Case 3</a></li>
            <li><a href="#case-4">Case 4</a></li>
          </ul>
        </li>
        <li><a href="#bert-embryology胚胎学">BERT Embryology（胚胎学）</a></li>
        <li><a href="#pre-training-a-seq2seq-model">Pre-training a seq2seq model</a>
          <ul>
            <li><a href="#mass--bart">MASS / BART</a></li>
            <li><a href="#t5---comparison">T5 - Comparison</a></li>
          </ul>
        </li>
        <li><a href="#why-does-bert-work">Why does BERT work?</a></li>
        <li><a href="#more">More</a></li>
        <li><a href="#multi-lingual-bert">Multi-lingual BERT</a>
          <ul>
            <li><a href="#zero-shot-reading-comprehension">Zero-shot Reading Comprehension</a></li>
            <li><a href="#cross-lingual-alignment">Cross-lingual Alignment</a></li>
            <li><a href="#mean-reciprocal-rankmrr">Mean Reciprocal Rank（MRR）</a></li>
          </ul>
        </li>
        <li><a href="#一个神奇的实验">一个神奇的实验</a></li>
      </ul>
    </li>
    <li><a href="#gpt">GPT</a>
      <ul>
        <li><a href="#predict-next-token">Predict Next Token</a></li>
        <li><a href="#how-to-use-gpt">How to use GPT?</a>
          <ul>
            <li><a href="#few-shot-learning">“Few-shot” Learning</a></li>
            <li><a href="#one-shot-learning">“One-shot” Learning</a></li>
            <li><a href="#zero-shot-learning">“Zero-shot” Learning</a></li>
          </ul>
        </li>
        <li><a href="#more-1">More</a></li>
      </ul>
    </li>
    <li><a href="#beyond-text">Beyond Text</a>
      <ul>
        <li><a href="#image---simclr">Image - SimCLR</a></li>
        <li><a href="#image---byol">Image - BYOL</a></li>
        <li><a href="#speech">Speech</a></li>
        <li><a href="#speech-glue---superb">Speech GLUE - SUPERB</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div class="content" id="content"><p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/cover.jpg"
    data-srcset="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/cover.jpg, https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/cover.jpg 1.5x, https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/cover.jpg 2x"
    data-sizes="auto"
    alt="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/cover.jpg"
    title="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/cover.jpg" />
<a
  href="https://www.bilibili.com/video/BV1Wv411h7kN?p=71"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>李宏毅2021/2022春机器学习课程——Self-Supervised Learning</a></p>
<h1 id="自监督式学习self-supervised-learning">自监督式学习（Self-Supervised Learning）</h1>
<h2 id="芝麻街与进击的巨人">芝麻街与进击的巨人</h2>
<h3 id="芝麻街">芝麻街</h3>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-1/stavreal.png" style="zoom:50%;" />
<h3 id="进击的巨人bertolt-hoover">进击的巨人：Bertolt Hoover</h3>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-1/%E8%B4%9D%E7%89%B9%E9%9C%8D%E5%B0%94%E5%BE%B7%E8%83%A1%E4%BD%9B.png" style="zoom:50%;" />
<h3 id="主流模型参数量">主流模型参数量</h3>
<table>
<thead>
<tr>
<th style="text-align:center">Model</th>
<th style="text-align:center">Parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">ELMO</td>
<td style="text-align:center">94M</td>
</tr>
<tr>
<td style="text-align:center">BERT</td>
<td style="text-align:center">340M</td>
</tr>
<tr>
<td style="text-align:center">GPT-2</td>
<td style="text-align:center">1542M</td>
</tr>
<tr>
<td style="text-align:center">Megatron</td>
<td style="text-align:center">8B</td>
</tr>
<tr>
<td style="text-align:center">T5</td>
<td style="text-align:center">11B</td>
</tr>
<tr>
<td style="text-align:center">Turing NLG</td>
<td style="text-align:center">17B</td>
</tr>
<tr>
<td style="text-align:center">GPT-3</td>
<td style="text-align:center">175B</td>
</tr>
<tr>
<td style="text-align:center">Switch Transformer</td>
<td style="text-align:center">1.6T</td>
</tr>
</tbody>
</table>
<h2 id="bert">BERT</h2>
<h3 id="self-supervised-learning">Self-supervised Learning</h3>
<p>对于数据$x$，监督学习需要知道数据的标签$\hat{y}$，来让模型输出我们想要的$y$。而自监督学习没有标注，将$x$分为两部分，一部分$x&rsquo;$输入到模型得到$y$，另一部分$x&rsquo;&rsquo;$作为标签，然后让$y$和$x&rsquo;&rsquo;$越接近越好。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/self-supervised%20learning.png" style="zoom:50%;" />
<p>自监督学习可以看作是一种无监督学习的方法，无监督学习的范围很大，里面有很多不同的方法，为了明确说明现在说做的工作，就称为自监督学习。</p>
<h3 id="masked-token-prediction">Masked token prediction</h3>
<ul>
<li>[<a
  href="https://arxiv.org/abs/1810.04805"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (arxiv.org)</a></li>
</ul>
<p>BERT的架构和Transfromer Encoder相同，输入一排向量，输出另一排向量，一般用在文字处理上。</p>
<p>输入一串token（token是处理一段文字的单位，在中文里一般把一个方块字当作一个token。），随机盖住一些token，盖住token有两种方法：</p>
<ul>
<li>变为某个特殊的token</li>
<li>随机换为另一个token</li>
</ul>
<p>对BERT的输出序列分别做线性变换（乘矩阵），再做Softmax就得到了一个分布。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/masking%20input.png" style="zoom:50%;" />
<p>BERT不知道被盖住的部分是什么内容，但我们知道这部分内容，BERT学习的目标是输出和盖住的部分越接近越好。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/masking%20input2.png" style="zoom:50%;" />
<h3 id="next-sentence-prediction">Next Sentence Prediction</h3>
<p>从资料库中拿出两个句子，两个句子之间加入特殊的分隔符号[SEP]，再整个序号的最前面加[CLS]符号，整个序列输入BERT，看[CLS]对应的输出，[CLS]经过BERT的输出再经过线性变换后输出为Yes/No，代表这两个句子是不是相接的。</p>
<p><em>例：[CLS] I like cat. [SEP] He likes dog</em></p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/next%20sentence%20prediction.png" style="zoom:50%;" />
<ul>
<li>
<p>Next Sentence Prediction对于BERT接下来要做的事情可能无用</p>
<ul>
<li>
<p>[<a
  href="https://arxiv.org/abs/1907.11692"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>1907.11692] RoBERTa: A Robustly Optimized BERT Pretraining Approach (arxiv.org)</a></p>
<p>Next Sentence Prediction这个任务可能比较简单，BERT可能学习不到太多有用的东西。</p>
</li>
</ul>
</li>
<li>
<p><strong>SOP</strong>：Sentence order prediction Used in ALBERT</p>
<ul>
<li>[<a
  href="https://arxiv.org/abs/1909.11942"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>1909.11942] ALBERT: A Lite BERT for Self-supervised Learning of Language Representations (arxiv.org)</a></li>
</ul>
<p>两个句子本来就连在一起，人为拆分开，本来放在前面的句子作为Sentence 1，本来放在后面的句子作为Sentence 2，或本来放在前面的句子作为Sentence 2，本来放在后面的句子作为Sentence 1。然后让BERT去回答是哪一种顺序。</p>
</li>
</ul>
<h3 id="downstream-tasks">Downstream Tasks</h3>
<p>在训练BERT时，给了BERT两个任务：</p>
<ul>
<li>Masked token prediction</li>
<li>Next sentence prediction</li>
</ul>
<p>在训练BERT时，似乎仅仅在教BERT如何去做“填空题”，但BERT可以用在其他地方，BERT真正在下游任务（Downstream Tasks）中被使用，但需要少量有标注的数据。BERT经过微调（Fine-tune）可以去完成各种其他的任务。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/Fine-tune.png" style="zoom:50%;" />
<h3 id="glue">GLUE</h3>
<p>任务集GLUE（General Language Understanding Evaluation）共有9个任务，</p>
<ul>
<li>Corpus of Linguistic Acceptability（CoLA）</li>
<li>Stanford Sentiment Treebank（SST-2）</li>
<li>Microsoft Research Paraphrase Corpus（MRPC）</li>
<li>Quora Question Pairs（QQP）</li>
<li>Semantic Textual Similarity Benchmark（STS-B）</li>
<li>Multi-Genre Natural Language Inference（MNLI）</li>
<li>Question-answering NLI（QNLI）</li>
<li>Recognizing Textual Entailment（RTE）</li>
<li>Winograd NLI（WNLI）</li>
</ul>
<p>可以在这9个任务上分别微调模型得到9个模型，通过结果数值来判断模型的好坏。</p>
<p>中文版本的GLUE：</p>
<ul>
<li><a
  href="https://www.cluebenchmarks.com/"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>CLUE中文语言理解基准测评 (cluebenchmarks.com)</a></li>
</ul>
<h3 id="how-to-use-bert">How to use BERT</h3>
<h4 id="case-1">Case 1</h4>
<blockquote>
<ul>
<li>Input：sequence</li>
<li>Output：class</li>
<li>Example：Sentiment analysis</li>
</ul>
</blockquote>
<p>给BERT输入一个句子，前面放[CLS] token，对[CLS]输出的向量做线性变换，Softmax后输出class。需要提供大量的已标注的训练资料。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case1.png" style="zoom:50%;" />
<p>Linear部分是随机初始化，而BERT部分将学会了做“填空题”的BERT模型的参数拿来初始化。</p>
<h4 id="case-2">Case 2</h4>
<blockquote>
<ul>
<li>Input：sequence</li>
<li>Output：sequence</li>
<li>Example：POS tagging（词性标注）</li>
</ul>
</blockquote>
<p>给BERT输入一个句子，前面放[CLS] token，对句子里面每一个token输出的向量做线性变换，Softmax后输出每一个token的类别。需要提供已标注的训练资料。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case2.png" style="zoom:50%;" />
<h4 id="case-3">Case 3</h4>
<blockquote>
<ul>
<li>
<p>Input：two sequences</p>
</li>
<li>
<p>Output：a class</p>
</li>
<li>
<p>Example：Natural Language Inference（NLI）</p>
<p>前提：一个人骑马越过了一架坏掉的飞机，假设：这个人在一个小餐馆里面，输出：矛盾。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case3_eg.png" style="zoom:50%;" />
</li>
</ul>
</blockquote>
<p>输入两个句子，两个句子之间放[SEP] token，第一个句子前放[CLS] token，整串内容输入BERT，对[CLS]输出的向量做线性变换，Softmax后输出class。需要提供已标注的训练资料。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case3.png" style="zoom:50%;" />
<h4 id="case-4">Case 4</h4>
<blockquote>
<ul>
<li>
<p><strong>Extraction-based Question Answering</strong>（有限制的QA，答案一定能在文章中找到。）</p>
</li>
<li>
<p>Input：</p>
<ul>
<li>
<p>Document：$D={d_1,d_2,\cdots,d_N}$</p>
</li>
<li>
<p>Query：$Q={q_1,q_2,\cdots,q_M}$</p>
</li>
</ul>
<p><em>对于中文，$d_i$和$q_i$都是汉字。</em></p>
</li>
<li>
<p>Output：two integers$(s,e)$</p>
<ul>
<li>Answer：$A={d_s,\cdots,d_e}$</li>
</ul>
<p><em>输出两个正整数，代表答案的范围。</em></p>
</li>
</ul>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case4_eg.png" style="zoom:50%;" />
</blockquote>
<p>输入问题和文章，问题和文章之间放[SEP] token，问题前放[CLS] token，整串内容输入BERT。</p>
<p>文章的各个token输出的向量先和一个向量（橙）做内积，再对结果做Softmax，得到答案起始的位置。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case4_1.png" style="zoom:50%;" />
<p>文章的各个token输出的向量再和一个向量（蓝）做内积，再对结果做Softmax，得到答案结束的位置。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/case4_2.png" style="zoom:50%;" />
<p>以上只有和BERT输出做内积的两个向量是随机初始化的，即这两个向量是重头开始学习的。</p>
<h3 id="bert-embryology胚胎学">BERT Embryology（胚胎学）</h3>
<ul>
<li>[<a
  href="https://arxiv.org/abs/2010.02480"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>2010.02480] Pretrained Language Model Embryology: The Birth of ALBERT (arxiv.org)</a></li>
</ul>
<p>BERT的训练需要耗费大量的资源，BERT的训练资料大概是30亿个词，是哈利波特全集的3000倍。有没有什么方法去节省计算资源？</p>
<p>从观察BERT的训练过程开始，BERT在什么时候学会填什么样的词汇？他的填空能力是怎么增进的？</p>
<h3 id="pre-training-a-seq2seq-model">Pre-training a seq2seq model</h3>
<p>BERT只有预训练的Encoder。</p>
<p>Encoder和Decoder间通过Cross Atention连接起来，在Encoder的输入中故意加一些扰动，希望Decoder输出的句子和弄坏前的句子是一样的。</p>
<h4 id="mass--bart">MASS / BART</h4>
<ul>
<li>[<a
  href="https://arxiv.org/abs/1905.02450"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>1905.02450] MASS: Masked Sequence to Sequence Pre-training for Language Generation (arxiv.org)</a></li>
<li>[<a
  href="https://arxiv.org/abs/1910.13461"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>1910.13461] BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (arxiv.org)</a></li>
</ul>
<p>对Encoder的输入加一些扰动来弄坏原本的内容：</p>
<ul>
<li>盖住一些词</li>
<li>删掉一些词</li>
<li>打乱词顺序</li>
<li>词顺序旋转</li>
<li>混合</li>
</ul>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/MASS_BART.png" style="zoom:50%;" />
<h4 id="t5---comparison">T5 - Comparison</h4>
<ul>
<li>Transfer Text-to-Text Transformer（T5）</li>
</ul>
<p>T5在Colossal Clean Crawled Corpus（C4）进行训练，对比了多种弄坏的方法。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-2/T5.png" style="zoom:50%;" />
<h3 id="why-does-bert-work">Why does BERT work?</h3>
<p>将文字输入到BERT中，得到的输出向量称为<strong>embedding</strong>，代表了各个token的意思，有相似意思的token有着非常相似的embeddng。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/embedding.png" style="zoom:50%;" />
<p>在语言中常常有一词多义的情况，例如“吃苹果”的“果”和“苹果手机”的“果”的含义可能相差较大。通过Cosine Similarity计算“吃苹果”和“苹果手机”的Embedding的相似度，可以发现它们之间的相似度较低。</p>
<blockquote>
<p>&ldquo;You shall know a word by the company it keeps.&rdquo;</p>
</blockquote>
<p>一个词的意思可以从上下文看出来，BERT在做“填空题”的过程中所学习的内容也许就是根据上下文来预测当前被盖住的词汇。事实上，BERT之前已经有这样的方法：word embedding，word embedding中的CBOW就是把中间挖空然后预测内容。BERT所抽取出来的向量也叫做<strong>Contextualized word embedding</strong>。</p>
<p>现在尝试将BERT拿来做蛋白质分类、DNA分类、音乐分类。</p>
<ul>
<li>[<a
  href="https://arxiv.org/abs/2103.07162"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>2103.07162] Is BERT a Cross-Disciplinary Knowledge Learner? A Surprising Finding of Pre-trained Models&rsquo; Transferability (arxiv.org)</a></li>
</ul>
<p>以DNA分类为例：</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/DNA.png" style="zoom:50%;" />
<p>将DNA中的序列替换成文字，输入到BERT中，输出分类，当作是文章分类的任务来处理。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/DNA_2.png" style="zoom:50%;" />
<p>类似地，对于蛋白质，随意给各个氨基酸映射到词汇上，对于音乐，将各个音符映射到词汇上，得到了下面的结果：</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/Protein_DNA_music_res.png" style="zoom:50%;" />
<p>BERT的表现是比较好的，就算给BERT乱七八糟的句子，它可能也能把任务完成的比较好，这也说明BERT的表现可能并不完全来自于它“看得懂”文章这件事，关于BERT到底为什么好的问题可能还有很大的研究空间。</p>
<h3 id="more">More</h3>
<ul>
<li>[<a
  href="https://www.youtube.com/watch?v=1_gRK9EIQpc"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>DLHLP 2020] BERT and its family - Introduction and Fine-tune - YouTube</a></li>
<li>[<a
  href="https://www.youtube.com/watch?v=Bywo7m6ySlk"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>DLHLP 2020] BERT and its family - ELMo, BERT, GPT, XLNet, MASS, BART, UniLM, ELECTRA, and more - YouTube</a></li>
</ul>
<h3 id="multi-lingual-bert">Multi-lingual BERT</h3>
<p>在训练的时候，会拿各种各样的语言来给BERT做“填空题”，Multi-BERT使用了104种语言来训练。拿英文的QA资料去训练，Multi-BERT就会做中文的QA问题。</p>
<h4 id="zero-shot-reading-comprehension">Zero-shot Reading Comprehension</h4>
<p>在英文数据集SQuAD和中文数据集DRCD上：</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/zero-shot.png" style="zoom:50%;" />
<ul>
<li>[<a
  href="https://arxiv.org/abs/1909.09587"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>1909.09587] Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model (arxiv.org)</a></li>
</ul>
<h4 id="cross-lingual-alignment">Cross-lingual Alignment</h4>
<p>也许对Multi-lingual BERT来说，不同语言间没有什么差别。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/cross-lingual.png" style="zoom:50%;" />
<h4 id="mean-reciprocal-rankmrr">Mean Reciprocal Rank（MRR）</h4>
<p>MRR值越高，两个不同语言align的越好（同样意思但不同语言的词汇的向量比较接近）。</p>
<p>在1000k资料量下，相比200k资料量下的效果显著提升：</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/mrr1.png" style="zoom:50%;" />
<h3 id="一个神奇的实验">一个神奇的实验</h3>
<p>BERT可以让同样意思但不同语言的词汇的向量很接近，但在训练Multi-lingual BERT时，还是给BERT喂中文，它能够做中文填空，喂英文能够做英文填空，不会混在一起，给他喂英文他并没有填中文进去。说明来自不同语言的符号终究还是不一样，并没有完全抹掉语言的资讯。</p>
<p>把所有英文的embedding平均起来，再把所有中文的embedding平均起来，两者相减得到的向量就是中文和英文之间的差距。给Multi-lingual BERT一句英文，得到一串embedding，将embedding加上相减得到的向量，这些向量对于Multi-lingual BERT就变成了中文的句子，再让BERT去做“填空题”，就填出了中文的答案。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/wired.png" style="zoom:50%;" />
<p>语言的资讯还是藏在Multi-lingual BERT中：</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-3/unsupervised%20learning.png" style="zoom:50%;" />
<h2 id="gpt">GPT</h2>
<h3 id="predict-next-token">Predict Next Token</h3>
<p>GPT修改了BERT中模型的任务，GPT的任务是预测接下来的句子是什么。</p>
<p>对于训练资料“台湾大学”，在最前面加上[BOS] token，对于[BOS] token，GPT输出一个embedding，接下来用这个embedding预测下一个应该出现的“台”这个token。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/predict%20next%20sentence.png" style="zoom:50%;" />
<p>GPT与拿掉Cross attention后的Transformer Decoder结构类似。</p>
<p>GPT要预测下一个token，有生成的能力，GPT最知名的例子就是用GPT写了一篇关于独角兽的假新闻。</p>
<ul>
<li><a
  href="https://app.inferkit.com/demo"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>Demo – InferKit</a></li>
</ul>
<h3 id="how-to-use-gpt">How to use GPT?</h3>
<p>GPT有一个更“狂“的使用方式，和人类更接近。</p>
<h4 id="few-shot-learning">“Few-shot” Learning</h4>
<p>例如在进行外语考试时，首先会看题目的说明（<em>“&hellip;从A、B、C、D四个选项中选出最佳选项&hellip;”</em>），再会看一个例子（<em>“&hellip;衬衫的价格是9镑15便士，所以你选择&hellip;”</em>）。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/few-shot%20learning.png" style="zoom:50%;" />
<p>“Few-shot” Learning中完全没有Gradient Descent，GPT文献中将这种训练称为“In-context Learning”。</p>
<p>类似地，还有“One-shot” Learning、甚至“Zero-shot” Learning。</p>
<h4 id="one-shot-learning">“One-shot” Learning</h4>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/one-shot%20learning.png" style="zoom:50%;" />
<h4 id="zero-shot-learning">“Zero-shot” Learning</h4>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/zero-shot%20learning.png" style="zoom:50%;" />
<p>第三代GPT测试了42个任务：</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/GPT_benchmarks.png" style="zoom:50%;" />
<h3 id="more-1">More</h3>
<ul>
<li>[<a
  href="https://www.youtube.com/watch?v=DOG1L9lvsDY"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>DLHLP 2020] 來自獵人暗黑大陸的模型 GPT-3 - YouTube</a></li>
</ul>
<h2 id="beyond-text">Beyond Text</h2>
<p>不止NLP，在语音、图像上都可以用Self-Supervised Learning的技术。</p>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/beyond%20text.png" style="zoom:50%;" />
<h3 id="image---simclr">Image - SimCLR</h3>
<ul>
<li>[<a
  href="https://arxiv.org/abs/2002.05709"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>2002.05709] A Simple Framework for Contrastive Learning of Visual Representations (arxiv.org)</a></li>
<li><a
  href="https://github.com/google-research/simclr"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>google-research/simclr: SimCLRv2 - Big Self-Supervised Models are Strong Semi-Supervised Learners (github.com)</a></li>
</ul>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/SimCLR.png" style="zoom:50%;" />
<h3 id="image---byol">Image - BYOL</h3>
<ul>
<li>[<a
  href="https://arxiv.org/abs/2006.07733"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>2006.07733] Bootstrap your own latent: A new approach to self-supervised Learning (arxiv.org)</a></li>
</ul>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/BYOL.png" style="zoom:50%;" />
<h3 id="speech">Speech</h3>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/speech.png" style="zoom:50%;" />
<h3 id="speech-glue---superb">Speech GLUE - SUPERB</h3>
<p><strong>S</strong>peech processing <strong>U</strong>niversal <strong>PER</strong>formance <strong>B</strong>enchmark，包含了十多个下游任务，包含内容、说话的人、情感、语义等。</p>
<ul>
<li>Toolkit：<a
  href="https://github.com/s3prl/s3prl/"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>s3prl/s3prl: Self-Supervised Speech Pre-training and Representation Learning Toolkit. (github.com)</a></li>
</ul>
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/7/7-4/toolkit.png" style="zoom:50%;" />
<ul>
<li><a
  href="https://github.com/andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
  
>https://github.com/andi611/Self-Supervised-Speech-Pretraining-and-Representation-Learning</a></li>
</ul></div><div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2022-05-16&#32;18:08:00>更新于 May 16, 2022</span>
      </div>
      <div class="post-info-license"><span><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
    </div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a
  href="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-supervised-learning/index.md"
  
    title="阅读原始文档"
  
  
  
  
  
    class="link-to-markdown"
  
  
>阅读原始文档</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-supervised-learning/" data-title="李宏毅ML课程笔记——Self-Supervised Learning"><i class="fa-brands fa-twitter fa-fw"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-supervised-learning/"><i class="fa-brands fa-facebook-square fa-fw"></i></a>
  <a href="javascript:void(0);" title="分享到 WhatsApp" data-sharer="whatsapp" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-supervised-learning/" data-title="李宏毅ML课程笔记——Self-Supervised Learning" data-web><i class="fa-brands fa-whatsapp fa-fw"></i></a>
  <a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-supervised-learning/" data-title="李宏毅ML课程笔记——Self-Supervised Learning"><i data-svg-src="/lib/simple-icons/icons/line.min.svg"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-supervised-learning/" data-title="李宏毅ML课程笔记——Self-Supervised Learning"><i class="fa-brands fa-weibo fa-fw"></i></a>
  <a href="javascript:void(0);" title="分享到 Myspace" data-sharer="myspace" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-supervised-learning/" data-title="李宏毅ML课程笔记——Self-Supervised Learning" data-description=""><i data-svg-src="/lib/simple-icons/icons/myspace.min.svg"></i></a>
  <a href="javascript:void(0);" title="分享到 Blogger" data-sharer="blogger" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-supervised-learning/" data-title="李宏毅ML课程笔记——Self-Supervised Learning" data-description=""><i class="fa-brands fa-blogger fa-fw"></i></a>
  <a href="javascript:void(0);" title="分享到 Evernote" data-sharer="evernote" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0self-supervised-learning/" data-title="李宏毅ML课程笔记——Self-Supervised Learning"><i class="fa-brands fa-evernote fa-fw"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/" class="prev" rel="prev" title="李宏毅ML课程笔记——生成式对抗网络（GAN）"><i class="fa-solid fa-angle-left fa-fw"></i>李宏毅ML课程笔记——生成式对抗网络（GAN）</a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">
          由<a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreffer" title="Hugo %v">Hugo</a>
          ×
          <a href="https://github.com/Lruihao/FixIt" target="_blank" rel="external nofollow noopener noreffer" title="FixIt %v"><img class="fixit-icon" src="/images/fixit.svg" alt="FixIt logo" />&nbsp;FixIt</a>
          强力驱动
          
        </div><div class="footer-line copyright"><i class="fa-regular fa-copyright fa-fw"></i>
            <span itemprop="copyrightYear">2020 - 2022</span><span class="license footer-divider"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line beian"><span class="icp footer-divider">鄂ICP备20012765号</span></div></div>
  </footer></div><div class="widgets">
  <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
      <i class="fa-solid fa-arrow-up fa-fw"></i>
    </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
      <i class="fa-solid fa-comment fa-fw"></i>
    </a>
  </div><a
  href="https://github.com/yzhn16"
  
    title="在 GitHub 上查看源代码"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
    class="github-corner right d-none-mobile"
  
  
><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div>
</div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js" defer></script><script type="text/javascript" src="/lib/lunr/lunr.min.js" defer></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js" defer></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js" defer></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js" async defer></script><script type="text/javascript" src="/lib/sharer/sharer.min.js" async defer></script><script type="text/javascript" src="/lib/katex/katex.min.js" defer></script><script type="text/javascript" src="/lib/katex/auto-render.min.js" defer></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js" defer></script><script type="text/javascript" src="/lib/katex/mhchem.min.js" defer></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"enablePWA":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js" defer></script><script type="text/javascript" src="/js/_custom.min.js" defer></script></body>
</html>
