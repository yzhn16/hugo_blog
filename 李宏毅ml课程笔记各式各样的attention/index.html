<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
    <title>李宏毅ML课程笔记——各式各样的Attention - yzhn&#39;s Notes</title><meta name="author" content="">
<meta name="author-link" content="">
<meta name="description" content="李宏毅2021/2022春机器学习课程——各式各样的神奇的自注意力" />
<meta name="keywords" content="" /><meta itemprop="name" content="李宏毅ML课程笔记——各式各样的Attention">
<meta itemprop="description" content="李宏毅2021/2022春机器学习课程——各式各样的神奇的自注意力"><meta itemprop="datePublished" content="2022-04-30T19:29:00+00:00" />
<meta itemprop="dateModified" content="2022-04-30T19:29:00+00:00" />
<meta itemprop="wordCount" content="1958"><meta itemprop="image" content="/logo.png"/>
<meta itemprop="keywords" content="" /><meta property="og:title" content="李宏毅ML课程笔记——各式各样的Attention" />
<meta property="og:description" content="李宏毅2021/2022春机器学习课程——各式各样的神奇的自注意力" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84attention/" /><meta property="og:image" content="/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-30T19:29:00+00:00" />
<meta property="article:modified_time" content="2022-04-30T19:29:00+00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="/logo.png"/>

<meta name="twitter:title" content="李宏毅ML课程笔记——各式各样的Attention"/>
<meta name="twitter:description" content="李宏毅2021/2022春机器学习课程——各式各样的神奇的自注意力"/>
<meta name="application-name" content="FixIt">
<meta name="apple-mobile-web-app-title" content="FixIt"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#252627"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84attention/" /><link rel="prev" href="/2022%E6%8B%9B%E5%95%86%E9%93%B6%E8%A1%8Cfintech%E7%B2%BE%E8%8B%B1%E8%AE%AD%E7%BB%83%E8%90%A5-%E6%95%B0%E6%8D%AE%E8%B5%9B%E9%81%93%E9%A6%96%E6%97%A5%E5%9F%BA%E7%BA%BF/" /><link rel="next" href="/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%84%9F%E7%9F%A5%E6%9C%BA/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "李宏毅ML课程笔记——各式各样的Attention",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "\/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84attention\/"
    },"genre": "posts","wordcount":  1958 ,
    "url": "\/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84attention\/","datePublished": "2022-04-30T19:29:00+00:00","dateModified": "2022-04-30T19:29:00+00:00","publisher": {
      "@type": "Organization",
      "name": ""},"author": {
        "@type": "Person",
        "name": "作者"
      },"description": ""
  }
  </script></head>
  <body header-desktop="sticky" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script><div class="wrapper"><header class="desktop" id="header-desktop">
  <div class="header-wrapper" github-corner="right">
    <div class="header-title">
      <a href="/" title="yzhn&#39;s Notes"><span class="header-title-text">乘月返真</span></a><span class="header-subtitle">YZhn的个人空间</span></div>
    <nav>
      <ul class="menu"><li
              class="menu-item"
              
            >
              <a
                class="menu-link"
                href="/posts/"
                
                
              ><i class='fa-solid fa-archive fa-fw fa-sm'></i> 文章</a></li><li
              class="menu-item"
              
            >
              <a
                class="menu-link"
                href="/categories/"
                
                
              ><i class='fa-solid fa-th fa-fw fa-sm'></i> 分类</a></li><li
              class="menu-item"
              
            >
              <a
                class="menu-link"
                href="/tags/"
                
                
              ><i class='fa-solid fa-tags fa-fw fa-sm'></i> 标签</a></li><li
              class="menu-item"
              
            >
              <a
                class="menu-link"
                href="/friends/"
                
                
              ><i class='fa-solid fa-users fa-fw fa-sm'></i> 友链</a></li><li
              class="menu-item"
              
            >
              <a
                class="menu-link"
                href="/about"
                title="关于"
                
              ><i class='fa-solid fa-info-circle fa-fw fa-sm'></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="yzhn&#39;s Notes"><span class="header-title-text">乘月返真</span></a><span class="header-subtitle">YZhn的个人空间</span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                class="menu-link"
                href="/posts/"
                
                
              ><i class='fa-solid fa-archive fa-fw fa-sm'></i> 文章</a></li><li
              class="menu-item"
            ><a
                class="menu-link"
                href="/categories/"
                
                
              ><i class='fa-solid fa-th fa-fw fa-sm'></i> 分类</a></li><li
              class="menu-item"
            ><a
                class="menu-link"
                href="/tags/"
                
                
              ><i class='fa-solid fa-tags fa-fw fa-sm'></i> 标签</a></li><li
              class="menu-item"
            ><a
                class="menu-link"
                href="/friends/"
                
                
              ><i class='fa-solid fa-users fa-fw fa-sm'></i> 友链</a></li><li
              class="menu-item"
            ><a
                class="menu-link"
                href="/about"
                title="关于"
                
              ><i class='fa-solid fa-info-circle fa-fw fa-sm'></i> 关于</a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw"></i>
        </li></ul>
    </nav>
  </div>
</header>
<div class="search-dropdown desktop">
  <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
  <div id="search-dropdown-mobile"></div>
</div>
<main class="container-reverse" page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录</h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom">
    
  </aside>

  <article class="page single"><h1 class="single-title animate__animated animate__flipInX">李宏毅ML课程笔记——各式各样的Attention</h1><div class="post-meta">
      
      <div class="post-meta-line"><span title=2022-04-30&#32;19:29:00>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="April 30, 2022" >April 30, 2022</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 1958 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 4 分钟&nbsp;</div>
    </div><div class="featured-image"><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="/images/X-former.png"
    data-srcset="/images/X-former.png, /images/X-former.png 1.5x, /images/X-former.png 2x"
    data-sizes="auto"
    alt="/images/X-former.png"
    title="/images/X-former.png" /></div><div class="details toc" id="toc-static" kept="false">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
<ul>
<li><a href="#self-attention变型">Self-attention变型</a>
<ul>
<li><a href="#human-knowledge">Human knowledge</a>
<ul>
<li><a href="#local-attention-truncated-attention">Local Attention / Truncated Attention</a></li>
<li><a href="#stride-attention">Stride Attention</a></li>
<li><a href="#global-attention">Global Attention</a></li>
<li><a href="#papers">Papers</a></li>
</ul></li>
<li><a href="#clustering">Clustering</a>
<ul>
<li><a href="#reformer">Reformer</a>
<ul>
<li><a href="#步骤">步骤</a></li>
</ul></li>
</ul></li>
<li><a href="#learnable-patterns">Learnable Patterns</a>
<ul>
<li><a href="#sinkhorn-sorting-network">Sinkhorn Sorting Network</a></li>
</ul></li>
<li><a href="#representative-key">Representative key</a>
<ul>
<li><a href="#linformer">Linformer</a>
<ul>
<li><a href="#reduce-number-of-keys">Reduce Number of Keys</a></li>
</ul></li>
</ul></li>
<li><a href="#k-q-first-rightarrow-v-k-first">$k,q$ first $\rightarrow$ $v,k$ first</a>
<ul>
<li><a href="#忽略softmax的情况">忽略Softmax的情况</a></li>
<li><a href="#加回softmax">加回Softmax</a></li>
<li><a href="#self-attention中的-q-k-v">Self-attention中的$q$、$k$、$v$</a></li>
<li><a href="#实现">实现</a></li>
</ul></li>
<li><a href="#new-framework">New framework</a>
<ul>
<li><a href="#无需-q-k-产生attention-synthesizer">无需$q,k$产生Attention——Synthesizer</a></li>
</ul></li>
<li><a href="#attention-free">Attention-free</a></li>
<li><a href="#总结">总结</a></li>
</ul></li>
</ul>
</nav></div>
      </div><div class="content" id="content"><p><a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=51">李宏毅2021/2022春机器学习课程——各式各样的神奇的自注意力</a></p>

<h1 id="self-attention变型">Self-attention变型</h1>

<p>Sequence length=$N$，产生的$N$个key向量和$N$个query向量两两之间做Dot-product，共$N^2$平方次计算，得到一个$N\times N$的矩阵Attention Matrix，根据该矩阵对value向量加权求和。Self-attention往往是模型里面的一个小模块，当$N$很大时，模型的主要计算量都集中在Self-attention上，对于计算速度的优化往往都是用在影像上。</p>

<h2 id="human-knowledge">Human knowledge</h2>

<h3 id="local-attention-truncated-attention">Local Attention / Truncated Attention</h3>

<p>某些问题不用看完整的序列，只用看左右邻居的信息即可，将其他位置的信息设为0。</p>

<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/local%20attention.png" style="zoom:50%;" /></p>

<p>可以加快运算速度，但每次做Attention只能看到某个小范围的信息，和CNN的差别就不大了。</p>

<h3 id="stride-attention">Stride Attention</h3>

<p>与Local Attention类似，看更远的邻居，例如看三个位置之前和三个位置之后的信息。</p>

<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/stride%20attention.png" style="zoom:50%;" /></p>

<h3 id="global-attention">Global Attention</h3>

<p>在原始序列中加入一些特殊的token，代表该位置要做Global Attention，Global Attention会从序列中的每一个token去收集信息。</p>

<ul>
<li>Attend to every token -&gt; 收集所有的信息<br /></li>
<li>Attended by every token -&gt; 能获取全局的信息<br />
<br /></li>
</ul>

<p>Global Attention有两种做法，可以从原始序列中选择一些已有的字符（例如BERT中的[CLS]标志、句号等）作为token，或外加额外的token。</p>

<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/global%20attention.png" style="zoom:50%;" /></p>

<h3 id="papers">Papers</h3>

<ul>
<li>[<a href="https://arxiv.org/abs/2004.05150">2004.05150] Longformer: The Long-Document Transformer (arxiv.org)</a><br /></li>
<li>[<a href="https://arxiv.org/abs/2007.14062">2007.14062] Big Bird: Transformers for Longer Sequences (arxiv.org)</a><br />
<br /></li>
</ul>

<h2 id="clustering">Clustering</h2>

<p>在Attention矩阵，可能有些值很大，有些值特别小，可以直接把较小的权值置0，问题在于如何快速估计哪些地方的Attention值较高，而哪些地方的Attention值较低。</p>

<h3 id="reformer">Reformer</h3>

<ul>
<li><p><a href="https://openreview.net/forum?id=rkgNKkHtvB">Reformer: The Efficient Transformer | OpenReview</a></p></li>

<li><p>[<a href="https://arxiv.org/abs/2003.05997">2003.05997] Efficient Content-Based Sparse Attention with Routing Transformers (arxiv.org)</a></p></li>
</ul>

<h4 id="步骤">步骤</h4>

<ul>
<li>Step 1：对query和key做聚类<br />
<br /></li>
</ul>

<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/clustering1.png" style="zoom:50%;" /></p>

<p>聚类有很多可以加速的方法，对query和key做聚类时，会采取精度相对较低但速度很快的方法。</p>

<ul>
<li>Step 2 对同一Cluster的query和key计算Attention分数<br />
<br /></li>
</ul>

<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/clustering2.png" style="zoom:50%;" /></p>

<p>不属于同一类的直接将Attention值设为0。</p>

<h2 id="learnable-patterns">Learnable Patterns</h2>

<h3 id="sinkhorn-sorting-network">Sinkhorn Sorting Network</h3>

<ul>
<li>[<a href="https://arxiv.org/abs/2002.11296">2002.11296] Sparse Sinkhorn Attention (arxiv.org)</a><br />
<br /></li>
</ul>

<p>让机器去学习两个位置的向量要不要做Attention。</p>

<p>Sinkhorn Sorting Network里面有一个额外需要学习的矩阵，来决定哪些地方需要计算Attention。</p>

<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/sinkhorn%20sorting%20network.png" style="zoom:50%;" /></p>

<p>多个向量会共享一个矩阵以加快计算速度。（例如对于长度为100的输入，会分成10组，每组都是同一个矩阵。）</p>

<h2 id="representative-key">Representative key</h2>

<p>Attention矩阵中有很多冗余列，往往无需$N\times N$的Attention矩阵。</p>

<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/Linformer.png" style="zoom:50%;" /></p>

<h3 id="linformer">Linformer</h3>

<ul>
<li>[<a href="https://arxiv.org/abs/2006.04768">2006.04768] Linformer: Self-Attention with Linear Complexity (arxiv.org)</a><br />
<br /></li>
</ul>

<p>从$N$个key中选出最有代表性的$K$个key，只需算$N\times K$的Attention矩阵。</p>

<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/Linformer2.png" style="zoom:50%;" /></p>

<h4 id="reduce-number-of-keys">Reduce Number of Keys</h4>

<p><strong>Linformer</strong>中，对$N$的向量做<strong>线性组合</strong>：<br />
$$<br />
M<em>{d\times N}\times M</em>{N\times K}=M_{d\times K}<br />
$$<br />
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/Linformer3.png" style="zoom:50%;" /></p>

<p>在<strong>Compressed Attention</strong>中的处理方式是对输入的较长序列用CNN去处理，得到一个较长的序列。</p>

<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/compressed%20attention.png" style="zoom:50%;" /></p>

<ul>
<li>[<a href="https://arxiv.org/abs/1801.10198">1801.10198] Generating Wikipedia by Summarizing Long Sequences (arxiv.org)</a><br />
<br /></li>
</ul>

<h2 id="k-q-first-rightarrow-v-k-first">$k,q$ first $\rightarrow$ $v,k$ first</h2>

<h3 id="忽略softmax的情况">忽略Softmax的情况</h3>

<p>首先<strong>不考虑Softmax的计算</strong>，Attention的计算式为：<br />
$$<br />
O\approx VK^TQ<br />
$$<br />
<img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/attention.png" style="zoom:50%;" /></p>

<p>调整计算顺序，有：<br />
$$<br />
O\approx V[K^TQ]\rightarrow O\approx [VK^T]Q<br />
$$</p>

<ul>
<li>对于计算方法$O\approx V[K^TQ]$：<br />

<ul>
<li>$A=K^TQ$：$N\times d\times N$<br /></li>
<li>$O=VA$：$d&rsquo;\times N\times N$<br /></li>
<li>求和：$(d+d&rsquo;)N^2$<br /></li>
</ul></li>
<li>而对于计算方法$O\approx [VK^T]Q$：<br />

<ul>
<li>$M_1=VK^T$：$d&rsquo;\times N\times d$<br /></li>
<li>$M_2=M_1Q$：$d&rsquo;\times d\times N$<br /></li>
<li>求和：$2d&rsquo;dN$<br />
<br /></li>
</ul></li>
</ul>

<h3 id="加回softmax">加回Softmax</h3>

<p>已知存在一个$\phi$，使得有<a href="#实现">以下式子</a>成立：</p>

<blockquote>
<p>$$<br />
\exp(q\cdot k)\approx \phi(q)\cdot\phi(k)<br />
$$</p>
</blockquote>

<p>则对于原始的Self-attention计算，有：<br />
$$<br />
\begin{aligned}<br />
b^1=\sum<em>{i=1}^Na&rsquo;</em>{1,i}v^i&amp;=\sum<em>{i=1}^N\frac{\exp{(q^1\cdot k^i)}}{\sum</em>{j=1}^{N}\exp{(q^1\cdot k^j)}}v^i <br />
&amp;=\sum<em>{i=1}^N\frac{\phi(q^1)\cdot\phi(k^i)}{\sum</em>{j=1}^{N}\phi(q^1)\cdot\phi(k^j)}v^i <br />
&amp;=\frac{\sum<em>{i=1}^{N}[\phi(q^1)\cdot\phi(k^i)]v^i}{\sum</em>{j=1}^{N}\phi(q^1)\cdot\phi(k^j)}<br />
\end{aligned}<br />
$$<br />
其中，对于分母部分：<br />
$$<br />
\sum<em>{j=1}^{N}\phi(q^1)\cdot\phi(k^j)=\phi(q^1)\cdot\sum</em>{j=1}^{N}\phi(k^j)<br />
$$<br />
对于分子部分，由于：<br />
$$<br />
\phi(q^1)=<br />
\begin{bmatrix}<br />
q_1^1 <br />
q_2^1 <br />
\vdots<br />
\end{bmatrix}<br />
\quad<br />
\phi(k^1)=<br />
\begin{bmatrix}<br />
k_1^1 <br />
k<em>2^1 <br />
\vdots<br />
\end{bmatrix}<br />
$$<br />
则有：<br />
$$<br />
\begin{aligned}<br />
&amp;\quad\sum</em>{i=1}^N[\phi(q^1)\cdot \phi(k^i)]v^i <br />
&amp;=[\phi(q^1)\cdot\phi(k^1)]v^1+[\phi(q^1)\cdot\phi(k^2)]v^2+\cdots <br />
&amp;=(q_1^1k_1^1+q_2^1k_2^1+\cdots)v^1+(q_1^1k_1^2+q_2^1k_2^2+\cdots)v^2+\cdots <br />
&amp;=(q_1^1k_1^1v^1+q_2^1k_2^1v^1+\cdots)+(q_1^1k_1^2v^2+q_2^1k_2^2v^2+\cdots)+\cdots <br />
&amp;=q_1^1(k_1^1v^1+k_1^2v^2+\cdots)+q_2^1(k_2^1v^1+k_2^2v^2+\cdots)+\cdots \</p>

<p>\end{aligned}<br />
$$<br />
设$\phi(q^1)$的维度为$M$，则：</p>

<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/res1.png" style="zoom:50%;" /></p>

<p>即在分子中的$M$个向量中，每一个向量都是通过，拿出$\phi(k^1)$、$\phi(k^2)$、&hellip;、$\phi(k^N)$中的第$i$个分量，对$v^1$、$v^2$、&hellip;、$v^N$做加权和。</p>

<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/res2.png" style="zoom:50%;" /></p>

<p>可以看出，每次计算$b^i$时，除了$\phi(q^i)$以外，其他部分没有发生变化，这部分内容无需<strong>再重复计算</strong>。</p>

<h3 id="self-attention中的-q-k-v">Self-attention中的$q$、$k$、$v$</h3>

<p>计算$b^1$：</p>

<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/res3.png" style="zoom:50%;" /></p>

<p>产生的$M$个向量以及$\sum_{j=1}^{N}\phi(k^j)$在后面$b^2$、$b^3$、$b^4$的计算中无需再进行计算。</p>

<h3 id="实现">实现</h3>

<p>关于$\phi$的实现：</p>

<ul>
<li>[<a href="https://arxiv.org/abs/1812.01243">1812.01243] Efficient Attention: Attention with Linear Complexities (arxiv.org)</a><br /></li>
<li><a href="https://linear-transformers.com/">Linear Transformers (linear-transformers.com)</a><br /></li>
<li>[<a href="https://arxiv.org/abs/2103.02143">2103.02143] Random Feature Attention (arxiv.org)</a><br /></li>
<li>[<a href="https://arxiv.org/abs/2009.14794">2009.14794] Rethinking Attention with Performers (arxiv.org)</a><br />
<br /></li>
</ul>

<h2 id="new-framework">New framework</h2>

<h3 id="无需-q-k-产生attention-synthesizer">无需$q,k$产生Attention——Synthesizer</h3>

<p>将Attention矩阵作为网络的参数。<br />
$$<br />
\begin{bmatrix}<br />
\alpha<em>{1,1} &amp; \alpha</em>{1,2} &amp; \alpha<em>{1,3} &amp; \alpha</em>{1,4} <br />
\alpha<em>{1,2} &amp; \alpha</em>{2,2} &amp; \alpha<em>{2,3} &amp; \alpha</em>{2,4} <br />
\alpha<em>{1,3} &amp; \alpha</em>{2,3} &amp; \alpha<em>{3,3} &amp; \alpha</em>{3,4} <br />
\alpha<em>{1,4} &amp; \alpha</em>{2,4} &amp; \alpha<em>{3,4} &amp; \alpha</em>{4,4} <br />
\end{bmatrix}<br />
$$</p>

<h2 id="attention-free">Attention-free</h2>

<ul>
<li>[<a href="https://arxiv.org/abs/2105.03824">2105.03824] FNet: Mixing Tokens with Fourier Transforms (arxiv.org)</a><br /></li>
<li>[<a href="https://arxiv.org/abs/2105.08050">2105.08050] Pay Attention to MLPs (arxiv.org)</a><br /></li>
<li>[<a href="https://arxiv.org/abs/2105.01601">2105.01601] MLP-Mixer: An all-MLP Architecture for Vision (arxiv.org)</a><br />
<br /></li>
</ul>

<h2 id="总结">总结</h2>

<p><img src="https://hexo-img-meurice.oss-cn-beijing.aliyuncs.com/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B2021%E6%98%A5/5/summary.png" style="zoom:50%;" /></p></div><div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2022-04-30&#32;19:29:00>更新于 April 30, 2022</span>
      </div>
      <div class="post-info-license"><span><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
    </div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a
  href="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84attention/index.md"
  
    title="阅读原始文档"
  
  
  
  
  
    class="link-to-markdown"
  
  
>阅读原始文档</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84attention/" data-title="李宏毅ML课程笔记——各式各样的Attention"><i class="fa-brands fa-twitter fa-fw"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84attention/"><i class="fa-brands fa-facebook-square fa-fw"></i></a>
  <a href="javascript:void(0);" title="分享到 WhatsApp" data-sharer="whatsapp" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84attention/" data-title="李宏毅ML课程笔记——各式各样的Attention" data-web><i class="fa-brands fa-whatsapp fa-fw"></i></a>
  <a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84attention/" data-title="李宏毅ML课程笔记——各式各样的Attention"><i data-svg-src="/lib/simple-icons/icons/line.min.svg"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84attention/" data-title="李宏毅ML课程笔记——各式各样的Attention" data-image="/images/X-former.png"><i class="fa-brands fa-weibo fa-fw"></i></a>
  <a href="javascript:void(0);" title="分享到 Myspace" data-sharer="myspace" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84attention/" data-title="李宏毅ML课程笔记——各式各样的Attention" data-description=""><i data-svg-src="/lib/simple-icons/icons/myspace.min.svg"></i></a>
  <a href="javascript:void(0);" title="分享到 Blogger" data-sharer="blogger" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84attention/" data-title="李宏毅ML课程笔记——各式各样的Attention" data-description=""><i class="fa-brands fa-blogger fa-fw"></i></a>
  <a href="javascript:void(0);" title="分享到 Evernote" data-sharer="evernote" data-url="/%E6%9D%8E%E5%AE%8F%E6%AF%85ml%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84attention/" data-title="李宏毅ML课程笔记——各式各样的Attention"><i class="fa-brands fa-evernote fa-fw"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/2022%E6%8B%9B%E5%95%86%E9%93%B6%E8%A1%8Cfintech%E7%B2%BE%E8%8B%B1%E8%AE%AD%E7%BB%83%E8%90%A5-%E6%95%B0%E6%8D%AE%E8%B5%9B%E9%81%93%E9%A6%96%E6%97%A5%E5%9F%BA%E7%BA%BF/" class="prev" rel="prev" title="2022招商银行FinTech精英训练营 数据赛道首日基线"><i class="fa-solid fa-angle-left fa-fw"></i>2022招商银行FinTech精英训练营 数据赛道首日基线</a>
      <a href="/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%84%9F%E7%9F%A5%E6%9C%BA/" class="next" rel="next" title="统计学习——感知机">统计学习——感知机<i class="fa-solid fa-angle-right fa-fw"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">
          由<a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreffer" title="Hugo %v">Hugo</a>
          ×
          <a href="https://github.com/Lruihao/FixIt" target="_blank" rel="external nofollow noopener noreffer" title="FixIt %v"><img class="fixit-icon" src="/images/fixit.svg" alt="FixIt logo" />&nbsp;FixIt</a>
          强力驱动
          
        </div><div class="footer-line copyright"><i class="fa-regular fa-copyright fa-fw"></i>
            <span itemprop="copyrightYear">2020 - 2022</span><span class="license footer-divider"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line beian"><span class="icp footer-divider">鄂ICP备20012765号</span></div></div>
  </footer></div><div class="widgets">
  <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
      <i class="fa-solid fa-arrow-up fa-fw"></i>
    </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
      <i class="fa-solid fa-comment fa-fw"></i>
    </a>
  </div><a
  href="https://github.com/yzhn16"
  
    title="在 GitHub 上查看源代码"
  
  
    
    
    target="_blank"
  
  
    rel="external nofollow noopener noreffer"
  
  
  
    class="github-corner right d-none-mobile"
  
  
><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div>
</div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js" defer></script><script type="text/javascript" src="/lib/lunr/lunr.min.js" defer></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js" defer></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js" defer></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js" async defer></script><script type="text/javascript" src="/lib/twemoji/twemoji.min.js" defer></script><script type="text/javascript" src="/lib/sharer/sharer.min.js" async defer></script><script type="text/javascript" src="/lib/katex/katex.min.js" defer></script><script type="text/javascript" src="/lib/katex/auto-render.min.js" defer></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"enablePWA":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"$$","right":"$$"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"},{"display":false,"left":"$","right":"$"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"},"twemoji":true};</script><script type="text/javascript" src="/js/theme.min.js" defer></script><script type="text/javascript" src="/js/_custom.min.js" defer></script></body>
</html>
